{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting AI and ML Job Trends with SARIMA\n",
    "\n",
    "At this stage, we perform Sentiment and Context Analysis using NLP Techniques\n",
    "\n",
    "- ***Contextual Skill Analysis***: Uses Named Entity Recognition (NER) to understand how AI skills are described in job postings.\n",
    "- ***Sentiment Analysis***: Determines employer sentiment around AI skills (e.g., \"essential,\" \"preferred\") to assess demand urgency.\n",
    "\n",
    "Model used: **BERT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "from transformers import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/b_job_postings_with_labels.parquet\"\n",
    "job_postings = pd.read_parquet(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(job_postings):,} job postings loaded from {filename}\")\n",
    "job_postings.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job prediction using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /Users/mzitoh/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/mzitoh/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at /Users/mzitoh/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /Users/mzitoh/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/mzitoh/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/mzitoh/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Loaded 109,482,240 parameters in the TF 2.0 model.\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer and model for sequence classification (binary classification)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 8 samples\n",
      "Testing data: 2 samples\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the job descriptions\n",
    "def tokenize_function(data):\n",
    "    tokens = tokenizer(\n",
    "        [item['text'] for item in data],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    labels = [item['label'] for item in data]\n",
    "    return tokens, tf.convert_to_tensor(labels)\n",
    "\n",
    "\n",
    "# Convert data into a Dataset object\n",
    "data = job_postings[:10][[\"job_description\", \"label\"]]\n",
    "data = data.rename(columns={\"job_description\": \"text\"})\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_data = train_test_split['train']\n",
    "test_data = train_test_split['test']\n",
    "\n",
    "print(f\"Training data: {len(train_data):,} samples\")\n",
    "print(f\"Testing data: {len(test_data):,} samples\")\n",
    "\n",
    "# Prepare training and test datasets\n",
    "train_tokens, train_labels = tokenize_function(train_data)\n",
    "test_tokens, test_labels = tokenize_function(test_data)\n",
    "\n",
    "# Prepare TensorFlow Dataset objects\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_tokens), train_labels\n",
    ")).batch(8)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_tokens), test_labels\n",
    ")).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=3e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:45\u001b[0;36m\u001b[0m\n\u001b[0;31m    val_acc_metric.reset_states()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# Custom training loop\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=3e-5)\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs):\n",
    "\ttokens, labels = inputs\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\tlogits = model(tokens, training=True).logits\n",
    "\t\tloss = loss_fn(labels, logits)\n",
    "\tgradients = tape.gradient(loss, model.trainable_variables)\n",
    "\toptimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\ttrain_acc_metric.update_state(labels, logits)\n",
    "\treturn loss\n",
    "\n",
    "@tf.function\n",
    "def val_step(inputs):\n",
    "\ttokens, labels = inputs\n",
    "\tval_logits = model(tokens, training=False).logits\n",
    "\tval_acc_metric.update_state(labels, val_logits)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "\tprint(f\"\\nStart of epoch {epoch + 1}\")\n",
    "\t\n",
    "\t# Training\n",
    "\tfor step, batch in enumerate(train_dataset):\n",
    "\t\tloss = train_step(batch)\n",
    "\t\tif step % 50 == 0:\n",
    "\t\t\tprint(f\"Training loss at step {step}: {loss:.4f}\")\n",
    "\t\n",
    "\t# Validation\n",
    "\tfor batch in test_dataset:\n",
    "\t\tval_step(batch)\n",
    "\t\n",
    "\ttrain_acc = train_acc_metric.result()\n",
    "\tval_acc = val_acc_metric.result()\n",
    "\tprint(f\"Training accuracy: {train_acc:.4f}\")\n",
    "\tprint(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "\t\n",
    "\ttrain_acc_metric.reset_states()\n",
    "\tval_acc_metric.reset_states() Custom training loop\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=3e-5)\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs):\n",
    "\ttokens, labels = inputs\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\tlogits = model(tokens, training=True).logits\n",
    "\t\tloss = loss_fn(labels, logits)\n",
    "\tgradients = tape.gradient(loss, model.trainable_variables)\n",
    "\toptimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\ttrain_acc_metric.update_state(labels, logits)\n",
    "\treturn loss\n",
    "\n",
    "@tf.function\n",
    "def val_step(inputs):\n",
    "\ttokens, labels = inputs\n",
    "\tval_logits = model(tokens, training=False).logits\n",
    "\tval_acc_metric.update_state(labels, val_logits)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "\tprint(f\"\\nStart of epoch {epoch + 1}\")\n",
    "\t\n",
    "\t# Training\n",
    "\tfor step, batch in enumerate(train_dataset):\n",
    "\t\tloss = train_step(batch)\n",
    "\t\tif step % 50 == 0:\n",
    "\t\t\tprint(f\"Training loss at step {step}: {loss:.4f}\")\n",
    "\t\n",
    "\t# Validation\n",
    "\tfor batch in test_dataset:\n",
    "\t\tval_step(batch)\n",
    "\t\n",
    "\ttrain_acc = train_acc_metric.result()\n",
    "\tval_acc = val_acc_metric.result()\n",
    "\tprint(f\"Training accuracy: {train_acc:.4f}\")\n",
    "\tprint(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "\t\n",
    "\ttrain_acc_metric.reset_states()\n",
    "\tval_acc_metric.reset_states() Custom training loop\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=3e-5)\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs):\n",
    "\ttokens, labels = inputs\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\tlogits = model(tokens, training=True).logits\n",
    "\t\tloss = loss_fn(labels, logits)\n",
    "\tgradients = tape.gradient(loss, model.trainable_variables)\n",
    "\toptimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\ttrain_acc_metric.update_state(labels, logits)\n",
    "\treturn loss\n",
    "\n",
    "@tf.function\n",
    "def val_step(inputs):\n",
    "\ttokens, labels = inputs\n",
    "\tval_logits = model(tokens, training=False).logits\n",
    "\tval_acc_metric.update_state(labels, val_logits)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "\tprint(f\"\\nStart of epoch {epoch + 1}\")\n",
    "\t\n",
    "\t# Training\n",
    "\tfor step, batch in enumerate(train_dataset):\n",
    "\t\tloss = train_step(batch)\n",
    "\t\tif step % 50 == 0:\n",
    "\t\t\tprint(f\"Training loss at step {step}: {loss:.4f}\")\n",
    "\t\n",
    "\t# Validation\n",
    "\tfor batch in test_dataset:\n",
    "\t\tval_step(batch)\n",
    "\t\n",
    "\ttrain_acc = train_acc_metric.result()\n",
    "\tval_acc = val_acc_metric.result()\n",
    "\tprint(f\"Training accuracy: {train_acc:.4f}\")\n",
    "\tprint(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "\t\n",
    "\ttrain_acc_metric.reset_states()\n",
    "\tval_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = model.evaluate(test_dataset)\n",
    "print(f\"Evaluation Results: {results}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
