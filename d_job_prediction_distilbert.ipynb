{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting AI and ML Job Trends with SARIMA\n",
    "\n",
    "At this stage, we perform Text Classification using the **DistilBERT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/b_job_postings_with_labels.parquet\"\n",
    "job_postings = pd.read_parquet(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,296,381 job postings loaded from data/b_job_postings_with_labels.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last_processed_time</th>\n",
       "      <th>got_summary</th>\n",
       "      <th>got_ner</th>\n",
       "      <th>is_being_worked</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>job_location</th>\n",
       "      <th>first_seen</th>\n",
       "      <th>search_city</th>\n",
       "      <th>search_country</th>\n",
       "      <th>search_position</th>\n",
       "      <th>job_level</th>\n",
       "      <th>job_type</th>\n",
       "      <th>job_skills</th>\n",
       "      <th>skills_count</th>\n",
       "      <th>job_description</th>\n",
       "      <th>keyword_count</th>\n",
       "      <th>keyword_likelihood</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>796981</th>\n",
       "      <td>2024-01-20 03:26:50.076097+00</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>senior land development engineer</td>\n",
       "      <td>stantec</td>\n",
       "      <td>london, ontario, canada</td>\n",
       "      <td>2024-01-14</td>\n",
       "      <td>london</td>\n",
       "      <td>canada</td>\n",
       "      <td>civil engineering technician</td>\n",
       "      <td>mid senior</td>\n",
       "      <td>onsite</td>\n",
       "      <td>[land development, site grading, site servicin...</td>\n",
       "      <td>35</td>\n",
       "      <td>senior land development engineer civil enginee...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233254</th>\n",
       "      <td>2024-01-21 04:31:36.376668+00</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>administrator</td>\n",
       "      <td>cld recruitment (leeds) ltd</td>\n",
       "      <td>leeds, england, united kingdom</td>\n",
       "      <td>2024-01-14</td>\n",
       "      <td>maidstone</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>historic-site administrator</td>\n",
       "      <td>mid senior</td>\n",
       "      <td>onsite</td>\n",
       "      <td>[mail processing, mail distribution, stationer...</td>\n",
       "      <td>15</td>\n",
       "      <td>administrator historic-site administrator cld ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524186</th>\n",
       "      <td>2024-01-19 09:45:09.215838+00</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>acute care rn - intensive care unit *night shi...</td>\n",
       "      <td>health ecareers</td>\n",
       "      <td>edmonds, wa</td>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>everett</td>\n",
       "      <td>united states</td>\n",
       "      <td>anesthesiologist</td>\n",
       "      <td>mid senior</td>\n",
       "      <td>onsite</td>\n",
       "      <td>[nursing, patient care, national provider bls,...</td>\n",
       "      <td>25</td>\n",
       "      <td>acute care rn - intensive care unit *night shi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676916</th>\n",
       "      <td>2024-01-19 09:45:09.215838+00</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>yard associate (skillbridge)</td>\n",
       "      <td>armor initiative</td>\n",
       "      <td>jacksonville, fl</td>\n",
       "      <td>2024-01-13</td>\n",
       "      <td>jacksonville</td>\n",
       "      <td>united states</td>\n",
       "      <td>orderly</td>\n",
       "      <td>associate</td>\n",
       "      <td>onsite</td>\n",
       "      <td>[yard associate, heavy equipment cleaning, yar...</td>\n",
       "      <td>18</td>\n",
       "      <td>yard associate (skillbridge) orderly armor ini...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71753</th>\n",
       "      <td>2024-01-19 15:38:31.076459+00</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>surgical technologist certified / non certifie...</td>\n",
       "      <td>mclaren health care</td>\n",
       "      <td>mount clemens, mi</td>\n",
       "      <td>2024-01-16</td>\n",
       "      <td>macomb</td>\n",
       "      <td>united states</td>\n",
       "      <td>polysomnographic technician</td>\n",
       "      <td>mid senior</td>\n",
       "      <td>onsite</td>\n",
       "      <td>[surgical technologist, surgical technology, b...</td>\n",
       "      <td>21</td>\n",
       "      <td>surgical technologist certified / non certifie...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   last_processed_time got_summary got_ner is_being_worked  \\\n",
       "796981   2024-01-20 03:26:50.076097+00           t       t               f   \n",
       "1233254  2024-01-21 04:31:36.376668+00           t       t               f   \n",
       "524186   2024-01-19 09:45:09.215838+00           t       t               f   \n",
       "676916   2024-01-19 09:45:09.215838+00           t       t               f   \n",
       "71753    2024-01-19 15:38:31.076459+00           t       t               f   \n",
       "\n",
       "                                                 job_title  \\\n",
       "796981                    senior land development engineer   \n",
       "1233254                                      administrator   \n",
       "524186   acute care rn - intensive care unit *night shi...   \n",
       "676916                        yard associate (skillbridge)   \n",
       "71753    surgical technologist certified / non certifie...   \n",
       "\n",
       "                             company                    job_location  \\\n",
       "796981                       stantec         london, ontario, canada   \n",
       "1233254  cld recruitment (leeds) ltd  leeds, england, united kingdom   \n",
       "524186               health ecareers                     edmonds, wa   \n",
       "676916              armor initiative                jacksonville, fl   \n",
       "71753            mclaren health care               mount clemens, mi   \n",
       "\n",
       "         first_seen   search_city  search_country  \\\n",
       "796981   2024-01-14        london          canada   \n",
       "1233254  2024-01-14     maidstone  united kingdom   \n",
       "524186   2024-01-12       everett   united states   \n",
       "676916   2024-01-13  jacksonville   united states   \n",
       "71753    2024-01-16        macomb   united states   \n",
       "\n",
       "                      search_position   job_level job_type  \\\n",
       "796981   civil engineering technician  mid senior   onsite   \n",
       "1233254   historic-site administrator  mid senior   onsite   \n",
       "524186               anesthesiologist  mid senior   onsite   \n",
       "676916                        orderly   associate   onsite   \n",
       "71753     polysomnographic technician  mid senior   onsite   \n",
       "\n",
       "                                                job_skills  skills_count  \\\n",
       "796981   [land development, site grading, site servicin...            35   \n",
       "1233254  [mail processing, mail distribution, stationer...            15   \n",
       "524186   [nursing, patient care, national provider bls,...            25   \n",
       "676916   [yard associate, heavy equipment cleaning, yar...            18   \n",
       "71753    [surgical technologist, surgical technology, b...            21   \n",
       "\n",
       "                                           job_description  keyword_count  \\\n",
       "796981   senior land development engineer civil enginee...              0   \n",
       "1233254  administrator historic-site administrator cld ...              0   \n",
       "524186   acute care rn - intensive care unit *night shi...              0   \n",
       "676916   yard associate (skillbridge) orderly armor ini...              0   \n",
       "71753    surgical technologist certified / non certifie...              0   \n",
       "\n",
       "         keyword_likelihood  label  \n",
       "796981                    0      0  \n",
       "1233254                   0      0  \n",
       "524186                    0      0  \n",
       "676916                    0      0  \n",
       "71753                     0      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"{len(job_postings):,} job postings loaded from {filename}\")\n",
    "job_postings.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job prediction using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer for DistilBERT\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# DistilBERT model for classification\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tf_datasets(train_ds, test_ds, save_dir=\"./data/tf_datasets\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Save datasets\n",
    "    tf.data.Dataset.save(train_ds, os.path.join(save_dir, \"train\"))\n",
    "    tf.data.Dataset.save(test_ds, os.path.join(save_dir, \"test\"))\n",
    "\n",
    "    print(f\"Datasets saved to {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_datasets(job_postings, tokenizer):\n",
    "    # Tokenize the data\n",
    "    def tokenize_function(train_data):\n",
    "        return tokenizer(\n",
    "            train_data[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "            return_tensors=\"tf\",  # Ensure TF tensors\n",
    "        )\n",
    "\n",
    "    print(\"Tokenizing data and creating datasets...\")\n",
    "    # Convert data into a Dataset object\n",
    "    data = job_postings[[\"job_description\", \"label\"]]\n",
    "    data = data.rename(columns={\"job_description\": \"text\"})\n",
    "    dataset = Dataset.from_dict(data)\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "    train_data = train_test_split[\"train\"]\n",
    "    test_data = train_test_split[\"test\"]\n",
    "\n",
    "    # Prepare training and test datasets\n",
    "    train_encoded = train_data.map(tokenize_function, batched=True)\n",
    "    test_encoded = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "    print(f\"Training data: {len(train_encoded):,} samples\")\n",
    "    print(f\"Testing data: {len(test_encoded):,} samples\")\n",
    "\n",
    "    print(\"Creating TensorFlow datasets...\")\n",
    "    # Convert labels to tensors\n",
    "    train_labels = tf.convert_to_tensor(train_data[\"label\"], dtype=tf.int32)\n",
    "    test_labels = tf.convert_to_tensor(test_data[\"label\"], dtype=tf.int32)\n",
    "\n",
    "    # Create TensorFlow datasets\n",
    "    train_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                {\n",
    "                    \"input_ids\": train_encoded[\"input_ids\"],\n",
    "                    \"attention_mask\": train_encoded[\"attention_mask\"],\n",
    "                },\n",
    "                train_labels,\n",
    "            )\n",
    "        )\n",
    "        .shuffle(100)\n",
    "        .batch(8)\n",
    "    )\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": test_encoded[\"input_ids\"],\n",
    "                \"attention_mask\": test_encoded[\"attention_mask\"],\n",
    "            },\n",
    "            test_labels,\n",
    "        )\n",
    "    ).batch(16)\n",
    "\n",
    "    save_tf_datasets(train_dataset, test_dataset)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tf_datasets(job_postings=None, tokenizer=None, load_dir=\"./data/tf_datasets3\"):\n",
    "    try:\n",
    "        if not os.path.exists(load_dir):\n",
    "            raise FileNotFoundError\n",
    "            \n",
    "        train_dataset = tf.data.Dataset.load(os.path.join(load_dir, \"train\"))\n",
    "        test_dataset = tf.data.Dataset.load(os.path.join(load_dir, \"test\"))\n",
    "\n",
    "        print(\"Datasets loaded from disk\")\n",
    "\n",
    "    except (FileNotFoundError, NameError):\n",
    "        if job_postings is None or tokenizer is None:\n",
    "            raise ValueError(\n",
    "                \"job_postings and tokenizer required when datasets not found\"\n",
    "            )\n",
    "\n",
    "        print(\"Creating new datasets...\")\n",
    "        train_dataset, test_dataset = create_tf_datasets(job_postings, tokenizer)\n",
    "\n",
    "    # Reapply dataset operations\n",
    "    train_dataset = train_dataset.shuffle(100).batch(8)\n",
    "    test_dataset = test_dataset.batch(16)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new datasets...\n",
      "Tokenizing data and creating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8/8 [00:00<00:00, 533.91 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 437.43 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 8 samples\n",
      "Testing data: 2 samples\n",
      "Creating TensorFlow datasets...\n",
      "Datasets saved to ./data/tf_datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset = load_tf_datasets(job_postings[:10], tokenizer)\n",
    "\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model compilation\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"./results/tf_checkpoints/model.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_format=\"keras\",\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(patience=2, monitor=\"val_loss\"),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=\"./logs\", update_freq=\"batch\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/tf_keras/src/engine/training.py\", line 1381, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/tf_keras/src/engine/training.py\", line 1370, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 1672, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_file3htvhr93.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_filecfunk608.py\", line 17, in tf__call\n        distilbert_output = ag__.converted_call(ag__.ld(self).distilbert, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_file3htvhr93.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_filemkqqrgwc.py\", line 93, in tf__call\n        embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (ag__.ld(input_ids),), dict(inputs_embeds=ag__.ld(inputs_embeds)), fscope)\n    File \"/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_file6wdrdefs.py\", line 54, in tf__call\n        final_embeddings = ag__.converted_call(ag__.ld(self).LayerNorm, (), dict(inputs=ag__.ld(final_embeddings)), fscope)\n\n    ValueError: Exception encountered when calling layer 'tf_distil_bert_for_sequence_classification' (type TFDistilBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 801, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/transformers/models/distilbert/modeling_tf_distilbert.py\", line 809, in call  *\n            distilbert_output = self.distilbert(\n        File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_file3htvhr93.py\", line 40, in tf__run_call_with_unpacked_inputs\n            raise\n        File \"/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_filemkqqrgwc.py\", line 93, in tf__call\n            embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (ag__.ld(input_ids),), dict(inputs_embeds=ag__.ld(inputs_embeds)), fscope)\n        File \"/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_file6wdrdefs.py\", line 54, in tf__call\n            final_embeddings = ag__.converted_call(ag__.ld(self).LayerNorm, (), dict(inputs=ag__.ld(final_embeddings)), fscope)\n    \n        ValueError: Exception encountered when calling layer 'distilbert' (type TFDistilBertMainLayer).\n        \n        in user code:\n        \n            File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 801, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/transformers/models/distilbert/modeling_tf_distilbert.py\", line 454, in call  *\n                embedding_output = self.embeddings(input_ids, inputs_embeds=inputs_embeds)  # (bs, seq_length, dim)\n            File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_file6wdrdefs.py\", line 54, in tf__call\n                final_embeddings = ag__.converted_call(ag__.ld(self).LayerNorm, (), dict(inputs=ag__.ld(final_embeddings)), fscope)\n        \n            ValueError: Exception encountered when calling layer 'embeddings' (type TFEmbeddings).\n            \n            in user code:\n            \n                File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/transformers/models/distilbert/modeling_tf_distilbert.py\", line 119, in call  *\n                    final_embeddings = self.LayerNorm(inputs=final_embeddings)\n                File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n            \n                ValueError: Exception encountered when calling layer 'LayerNorm' (type LayerNormalization).\n                \n                Cannot reshape a tensor with 768 elements to shape [1,1,128,1] (128 elements) for '{{node tf_distil_bert_for_sequence_classification/distilbert/embeddings/LayerNorm/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](tf_distil_bert_for_sequence_classification/distilbert/embeddings/LayerNorm/Reshape/ReadVariableOp, tf_distil_bert_for_sequence_classification/distilbert/embeddings/LayerNorm/Reshape/shape)' with input shapes: [768], [4] and with input tensors computed as partial shapes: input[1] = [1,1,128,1].\n                \n                Call arguments received by layer 'LayerNorm' (type LayerNormalization):\n                  • inputs=tf.Tensor(shape=(None, None, 128, 768), dtype=float32)\n            \n            \n            Call arguments received by layer 'embeddings' (type TFEmbeddings):\n              • input_ids=tf.Tensor(shape=(None, None, 128), dtype=int32)\n              • position_ids=None\n              • inputs_embeds=None\n              • training=True\n        \n        \n        Call arguments received by layer 'distilbert' (type TFDistilBertMainLayer):\n          • input_ids=tf.Tensor(shape=(None, None, 128), dtype=int32)\n          • attention_mask=tf.Tensor(shape=(None, None, 128), dtype=int32)\n          • head_mask=None\n          • inputs_embeds=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer 'tf_distil_bert_for_sequence_classification' (type TFDistilBertForSequenceClassification):\n      • input_ids={'input_ids': 'tf.Tensor(shape=(None, None, 128), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(None, None, 128), dtype=int32)'}\n      • attention_mask=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Model training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:1229\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1228\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_fileugx8d5_t.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:1672\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1670\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1672\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_using_dummy_loss:\n\u001b[1;32m   1674\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiled_loss(y_pred\u001b[38;5;241m.\u001b[39mloss, y_pred\u001b[38;5;241m.\u001b[39mloss, sample_weight, regularization_losses\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses)\n",
      "File \u001b[0;32m/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_file3htvhr93.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(func), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m),), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_filecfunk608.py:17\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[1;32m     15\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     16\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 17\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(distilbert_output)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(hidden_state)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_file3htvhr93.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(func), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m),), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_filemkqqrgwc.py:93\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m     91\u001b[0m     head_mask \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mnum_hidden_layers\n\u001b[1;32m     92\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(head_mask) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, if_body_4, else_body_4, get_state_4, set_state_4, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhead_mask\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m tfmr_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mtransformer, (ag__\u001b[38;5;241m.\u001b[39mld(embedding_output), ag__\u001b[38;5;241m.\u001b[39mld(attention_mask), ag__\u001b[38;5;241m.\u001b[39mld(head_mask), ag__\u001b[38;5;241m.\u001b[39mld(output_attentions), ag__\u001b[38;5;241m.\u001b[39mld(output_hidden_states), ag__\u001b[38;5;241m.\u001b[39mld(return_dict)), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_file6wdrdefs.py:54\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_ids, position_ids, inputs_embeds, training)\u001b[0m\n\u001b[1;32m     52\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mgather, (), \u001b[38;5;28mdict\u001b[39m(params\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mposition_embeddings, indices\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(position_ids)), fscope)\n\u001b[1;32m     53\u001b[0m final_embeddings \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(inputs_embeds) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(position_embeds)\n\u001b[0;32m---> 54\u001b[0m final_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_embeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m final_embeddings \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdropout, (), \u001b[38;5;28mdict\u001b[39m(inputs\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(final_embeddings), training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/tf_keras/src/engine/training.py\", line 1381, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/tf_keras/src/engine/training.py\", line 1370, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 1672, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_file3htvhr93.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_filecfunk608.py\", line 17, in tf__call\n        distilbert_output = ag__.converted_call(ag__.ld(self).distilbert, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_file3htvhr93.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_filemkqqrgwc.py\", line 93, in tf__call\n        embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (ag__.ld(input_ids),), dict(inputs_embeds=ag__.ld(inputs_embeds)), fscope)\n    File \"/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_file6wdrdefs.py\", line 54, in tf__call\n        final_embeddings = ag__.converted_call(ag__.ld(self).LayerNorm, (), dict(inputs=ag__.ld(final_embeddings)), fscope)\n\n    ValueError: Exception encountered when calling layer 'tf_distil_bert_for_sequence_classification' (type TFDistilBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 801, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/transformers/models/distilbert/modeling_tf_distilbert.py\", line 809, in call  *\n            distilbert_output = self.distilbert(\n        File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_file3htvhr93.py\", line 40, in tf__run_call_with_unpacked_inputs\n            raise\n        File \"/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_filemkqqrgwc.py\", line 93, in tf__call\n            embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (ag__.ld(input_ids),), dict(inputs_embeds=ag__.ld(inputs_embeds)), fscope)\n        File \"/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_file6wdrdefs.py\", line 54, in tf__call\n            final_embeddings = ag__.converted_call(ag__.ld(self).LayerNorm, (), dict(inputs=ag__.ld(final_embeddings)), fscope)\n    \n        ValueError: Exception encountered when calling layer 'distilbert' (type TFDistilBertMainLayer).\n        \n        in user code:\n        \n            File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 801, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/transformers/models/distilbert/modeling_tf_distilbert.py\", line 454, in call  *\n                embedding_output = self.embeddings(input_ids, inputs_embeds=inputs_embeds)  # (bs, seq_length, dim)\n            File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/var/folders/kd/9h3yrkps2vj0t9zs17vt349c0000gn/T/__autograph_generated_file6wdrdefs.py\", line 54, in tf__call\n                final_embeddings = ag__.converted_call(ag__.ld(self).LayerNorm, (), dict(inputs=ag__.ld(final_embeddings)), fscope)\n        \n            ValueError: Exception encountered when calling layer 'embeddings' (type TFEmbeddings).\n            \n            in user code:\n            \n                File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/transformers/models/distilbert/modeling_tf_distilbert.py\", line 119, in call  *\n                    final_embeddings = self.LayerNorm(inputs=final_embeddings)\n                File \"/Users/mzitoh/Desktop/USD-Source/data_mining/ai-ml-job-trends/.venv/lib/python3.11/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n            \n                ValueError: Exception encountered when calling layer 'LayerNorm' (type LayerNormalization).\n                \n                Cannot reshape a tensor with 768 elements to shape [1,1,128,1] (128 elements) for '{{node tf_distil_bert_for_sequence_classification/distilbert/embeddings/LayerNorm/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](tf_distil_bert_for_sequence_classification/distilbert/embeddings/LayerNorm/Reshape/ReadVariableOp, tf_distil_bert_for_sequence_classification/distilbert/embeddings/LayerNorm/Reshape/shape)' with input shapes: [768], [4] and with input tensors computed as partial shapes: input[1] = [1,1,128,1].\n                \n                Call arguments received by layer 'LayerNorm' (type LayerNormalization):\n                  • inputs=tf.Tensor(shape=(None, None, 128, 768), dtype=float32)\n            \n            \n            Call arguments received by layer 'embeddings' (type TFEmbeddings):\n              • input_ids=tf.Tensor(shape=(None, None, 128), dtype=int32)\n              • position_ids=None\n              • inputs_embeds=None\n              • training=True\n        \n        \n        Call arguments received by layer 'distilbert' (type TFDistilBertMainLayer):\n          • input_ids=tf.Tensor(shape=(None, None, 128), dtype=int32)\n          • attention_mask=tf.Tensor(shape=(None, None, 128), dtype=int32)\n          • head_mask=None\n          • inputs_embeds=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer 'tf_distil_bert_for_sequence_classification' (type TFDistilBertForSequenceClassification):\n      • input_ids={'input_ids': 'tf.Tensor(shape=(None, None, 128), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(None, None, 128), dtype=int32)'}\n      • attention_mask=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=3,\n",
    "    validation_data=test_dataset,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {results[0]}, Test Accuracy: {results[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on a sample job description\n",
    "sample_job = \"Looking for a data scientist skilled in machine learning and data analysis.\"\n",
    "encoded_input = tokenizer(sample_job, return_tensors=\"tf\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "output = model(encoded_input)\n",
    "prediction = tf.argmax(output.logits, axis=-1).numpy()[0]\n",
    "\n",
    "print(f\"Predicted label: {'AI skills required' if prediction == 1 else 'No AI skills required'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
